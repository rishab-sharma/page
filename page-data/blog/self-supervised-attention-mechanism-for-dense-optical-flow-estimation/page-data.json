{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/self-supervised-attention-mechanism-for-dense-optical-flow-estimation","result":{"data":{"markdownRemark":{"excerpt":"Before we get into what self-supervised attention means, let’s get an intuition of optical flow estimation and how it serves as an approach…","html":"<p>Before we get into what self-supervised attention means, let’s get an intuition of optical flow estimation and how it serves as an approach for tracking objects by both humans and computer vision systems.</p>\n<hr>\n<p>It is a consensus that object tracking is a fundamental ability that is developed by a human baby at an early age of about two to three months. However, at the level of neurophysiology, the actual working mechanism of the human visual system still remains somewhat obscure. Similar to the human visual system, computer vision systems also widely use tracking for various applications like video surveillance and autonomous driving. The objective of a tracking algorithm is to relocate a particular set of objects in a given video sequence that it has identified in the initial frames. In the research literature related to tracking, it is studied under two major categories namely Visual Object Tracking (VOT) and Semisupervised Video Object Segmentation (Semi-VOS). The first one (VOT) aims to track objects by relocalizing object bounding boxes throughout the video sequence. Whereas the latter (Semi-VOS) tracks objects at a more fine-grained level through a pixel-level segmentation mask. In this blog, we will discuss the original idea behind the latter approach i.e Dense Optical Flow Estimation and how this kind of dense tracking approach is achieved through self-supervised attention mechanisms.</p>\n<h2 id=\"Dense-Optical-Flow-Estimation\" style=\"position:relative;\"><a href=\"#Dense-Optical-Flow-Estimation\" aria-label=\"Dense Optical Flow Estimation permalink\" class=\"gatsby-remark-autolink before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Dense Optical Flow Estimation</h2>\n<p>Dense optical flow is one of the categories of the concept of Optical flow. Optical flow can be defined as the motion of objects between consecutive frames of a video sequence, as a consequence of relative motion between the object and camera. To explain the same in a scientific language, we can say that optical flow is the distribution of apparent velocities of movement of brightness patterns in an image that arises from the relative motion of objects and the viewer. Optical flow is studied as Sparse optical flow and Dense optical flow. Sparse optical flow derives flow vectors of only a few interesting pixels in the frame that either depict some edge or corner of an object. On the other hand, Dense optical flow derives flow vectors of all the pixels in a given frame, thus giving a higher accuracy at the cost of more computation and less speed.</p>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/page/static/63ddad865935691cfab1ac69006367d6/8c557/dofe1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 50%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsSAAALEgHS3X78AAACM0lEQVQozzXR/08ScRgH8Pfn+Bycx33VSpdBom46tPl9AmWApGjTLXPL0pq2Wc3KVCaCHSAWHEfCLFOQxJmtuez3/r+OtbZnzw/P9nrvvT1oaXc5Wl3Xnc5Gh6NneGgk7AtN+nx3Bzu6Ots62lUrKwISiARGAhVhk1EnE16BoEJGv7ffE/J6Rj29g0OBcHBiJjwaHul0t99sa73hdIiUCoAdjADWHAlWsZbCmlIxM8dnbocfjoUfhCanQyvL8+MTE/dnnwx7/bd6hzq7+wSW5YE6Qs1NB3qs7i612SM7fTI4haiIbD59ufJsaXFuK7q0sPDY3edfWVt9vjgXDAb9gXuS1cb9x8yjabqRUqc0ueWOWUEhDcie5tPHevZE378oLkfWuwf9yS+aUU3vlnbfpdbqRTsFOIbaAMvYKNWSdVeb7CACkUSzduKsmDwvamd7mcuj+ddxhzsQreQSP/Kpn3uRw4SkSAAIobAxeDUAbZq4VIbwFmpnLSK0k0K8kt8+2TOqxWg2uRRZ3z0v5i4PjN9ftUpWUuUaBoMAxaYLs140XwPHgxcIZ0e8ZMTLhlYpRPVkomro1Xz626ePF5/1Xwfb5YxUX8NMI7H8oRhoJC0uoiimhF1EDZeN+HE+dpR7kY5tFHe0svEml1rfT5v3t7ltQRZruIlYpiyEv8IIDYxdgixB4MFZsVXSt47Mya4VUrHDzIfvhZ3TfKyUeX+sr+bi/zAhTG2Z9W0UAguBmK8HR/4CP0KLUUKIRWIAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Image\"\n        title=\"Image\"\n        src=\"/page/static/63ddad865935691cfab1ac69006367d6/fcda8/dofe1.png\"\n        srcset=\"/page/static/63ddad865935691cfab1ac69006367d6/12f09/dofe1.png 148w,\n/page/static/63ddad865935691cfab1ac69006367d6/e4a3f/dofe1.png 295w,\n/page/static/63ddad865935691cfab1ac69006367d6/fcda8/dofe1.png 590w,\n/page/static/63ddad865935691cfab1ac69006367d6/8c557/dofe1.png 700w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">Image</figcaption>\n  </figure></p>\n<p>Dense optical flow computes one optical flow vector per pixel for every frame in the video sequence. Unlike sparse optical flow, this approach gives a more suitable output for applications such as video segmentation and structural learning from motion. Dense optical flow can be implemented by various methods. Among them, one of the simplest to use algorithm is the Farneback method. It is based on Gunner Farneback’s algorithm which is explained in “Two-Frame Motion Estimation Based on Polynomial Expansion” by Gunner Farneback in 2003. OpenCV provides the code function to this algorithm to find the dense optical flow. For a quick experience of what Farneback’s algorithm is, run the following code snippet.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">import cv2\nimport numpy as np\nfrom glob import glob\nimport requests \nimport os\n\ndef get_video(video_url):\n  r = requests.get(video_url, stream = True) \n  with open(&#39;./vid.mp4&#39;, &#39;wb&#39;) as f: \n    for chunk in r.iter_content(chunk_size = 1024*1024): \n      if chunk: \n        f.write(chunk)\n\ndef estimate_optical_flow(video, frame_dir):\n  ret, frame1 = video.read()\n  prvs = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n  hsv = np.zeros_like(frame1)\n  hsv[...,1] = 255\n  seq = 1\n  while(1):\n    ret, frame2 = video.read()\n    next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n\n    flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n\n    mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n    hsv[...,0] = ang*180/np.pi/2\n    hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n    rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n\n    cv2.imwrite(f&quot;{frame_dir}/{seq}.png&quot;,rgb)\n    seq+=1\n    if seq==200:\n      break\n  video.release()\n\ndef generate_output(frame_dir):\n  img_array = []\n  for filename in sorted(glob(f&quot;{frame_dir}/*.png&quot;)):\n    img = cv2.imread(filename)\n    height, width, layers = img.shape\n    size = (width,height)\n    img_array.append(img)\n    \n  fourcc = cv2.VideoWriter_fourcc(*&#39;mp4v&#39;)\n  out = cv2.VideoWriter(&#39;./Dense-optical-flow.mp4&#39;, fourcc, 20.0, size)\n  \n  for i in range(len(img_array)):\n    out.write(img_array[i])\n  out.release()\n\ndef main():\n  video_url = &quot;https://viratdata.org/video/VIRAT_S_010204_05_000856_000890.mp4&quot;\n  get_video(video_url)\n  video = cv2.VideoCapture(&quot;./vid.mp4&quot;)\n  if not os.path.exists(&#39;./frames&#39;):\n    os.mkdir(&#39;./frames&#39;)\n  estimate_optical_flow(video, &#39;./frames&#39;)\n  generate_output(&#39;./frames&#39;)\n\nif __name__ == &quot;__main__&quot;:\n  main()</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<p>After running the above code, you will get the following output (right side) in a video(Dense-optical-flow.mp4)\n<img src=\"./images/dofe2.gif\" alt=\"GIF\"></p>\n<p>The Farneback algorithm is an effective technique to estimate the motion of certain image features by comparing two consecutive frames from a video sequence. The algorithm first uses the polynomial expansion transform to approximate the windows of image frames through the quadratic polynomials. Polynomial expansion transform is a signal transform designed exclusively in the spatial domain and can be used for signals of any dimensionality. The method observes the translation of the polynomial transforms to estimate displacement fields from polynomial expansion coefficients. This method then computes the dense optical flow after a series of iterative refinements. In the implementation code, the algorithm computes the direction and magnitude of optical flow from a two-channel array of flow vectors (dx/dt, dy/dt). The computed direction and magnitude are then visualized by the value of HSV color representation which is set to a maximum of 255 for optimal visibility.</p>\n<h2 id=\"Deep-Learning-for-Dense-Optical-Flow-Estimation\" style=\"position:relative;\"><a href=\"#Deep-Learning-for-Dense-Optical-Flow-Estimation\" aria-label=\"Deep Learning for Dense Optical Flow Estimation permalink\" class=\"gatsby-remark-autolink before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Deep Learning for Dense Optical Flow Estimation</h2>\n<p>Historically, the problem of optical flow is an optimization problem. After the recent developments in deep learning, many researchers have applied deep learning to solve this optimization problem by processing consecutive video frames as input to calculate the optical flow of the object in motion. Although these approaches just process two consecutive frames at a time, still the essence of a video is captured in these two frames. The main thing that distinguishes videos from images is that videos possess a temporal structure in addition to the spatial structure of the images. However, videos also have other modalities such as sound, but they are of no use in this case. Therefore consecutive frame stream can be interpreted as a collection of images operating in a specific temporal resolution (fps). This means that data in a video is encoded not only spatially but also sequentially, which makes classifying videos quite interesting and yet challenging at the same time.\n<figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/page/static/eba41de34a111dca72585315811aff6f/8c557/dofe3.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 61.48648648648649%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsSAAALEgHS3X78AAAC1ElEQVQozy1ReUzScRz9/hDP4YnN5lq0bLZWm39VttnWyGytWq2sNmebORUCmS5t2mweIHK4oVKioLAMkwiUPKA2LY/SpfNoKYIXOvFAQFC8UfgG6h9ve5/t8/be+3zA0FD/wJ/fkgZxBoVczeMqWxR0ejEmABMaiMVGncTePBUadsLfz5ckrlNB+PXANraxPLy30uW0dkIr0KhHtZqxoqJCHA7HreC2trTl5rwBACAuJASAxADEF+Wi96oFLAjrzIt2dZ9qw8pz7FQ6N0GNUPxJIqNS6VejY5iMUulnGYmU7tpGIShwHQOeBSOeHq7xSWUVAcIvhnmoHRCvGq84duMOrID7nl9IK82nspJTSclpZKFIREmnHDmjsejwSJ9zfmjXSK2kTsD9yfVt84J+cl6XsLlycd8IJjWjI0P9IlHNwwf3yzhsZVszjUY9EiOR3ugbGC9Pd+zbDH75GuTM2wctjnaTU25wSM1OMDI0+O/vMJNRchqH43DKFIrm7KzXx50Tg0EaFjl0vsCoxZsgYcJmN65V6GGCzpEyBwGLwyst4xWXsPGxdzJf5dTXS4hE0rH4kg/qcZCPv1v8opiTYoYs087B9s53o/P8giPOYANyubziHY9KZ5EpmUQyRSgS5ubmHolRIR5hZ7zPerkPll5QIjNB6eJ+n+Ggf8kuXdr9adGD6Ylx9eiIgM+LuRZdymZ8Uypp1OPO4LIv8jQIOXSOotUSl2C2ZndEb2+ZdnyYcnx0xe5o7+jp7mYxmRHnIqt41Y1yBYWS4X6VC48CQVIIQNwUxxZEbMLYGfOWVi1YhfFGZ5YRgreF7LwCVl4+HX/r7vOkND5fSCC8PHRGQLgHGu+JCXTHTuXwhBCWb1k2DMNzOn382nrc3gIwGAwa7VRXT++Pzu6Ozl8zutlxtUapUjY1NklkDa0Sca+iSaVsm5ubhxDaLBaTXr8+O7O6btLC5f9fUGuCuwdrVgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Image\"\n        title=\"Image\"\n        src=\"/page/static/eba41de34a111dca72585315811aff6f/fcda8/dofe3.png\"\n        srcset=\"/page/static/eba41de34a111dca72585315811aff6f/12f09/dofe3.png 148w,\n/page/static/eba41de34a111dca72585315811aff6f/e4a3f/dofe3.png 295w,\n/page/static/eba41de34a111dca72585315811aff6f/fcda8/dofe3.png 590w,\n/page/static/eba41de34a111dca72585315811aff6f/8c557/dofe3.png 700w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">Image</figcaption>\n  </figure>\nGenerally, deep neural networks require a large amount of training data to learn and optimize the approximation functions. But in the case of optical flow estimation, training data is particularly hard to obtain. The major reason behind this is the difficulty of accurately labeling video footage for the exact motion of every point of an image to subpixel accuracy. Therefore to address the issue of labeling video data, computer graphics are used to simulate massive realistic worlds through instructions. As the instructions are known, the motion of every pixel in the video frame sequence is already known. Some of the recent research that attempts to solve the optical flow problems are PWC-Nets, ADLAB-PRFlow, and FlowNet. Optical flow is widely inherited by many applications like vehicle tracking and traffic analysis through object detection and multi-object tracking by feature-based optical flow techniques from either from a stationary camera or cameras attached to vehicles.</p>\n<h2 id=\"Self-Supervised-Deep-Learning-for-Tracking\" style=\"position:relative;\"><a href=\"#Self-Supervised-Deep-Learning-for-Tracking\" aria-label=\"Self Supervised Deep Learning for Tracking permalink\" class=\"gatsby-remark-autolink before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Self-Supervised Deep Learning for Tracking</h2>\n<p>As mentioned earlier, visual tracking is integral for many tasks like recognition, interaction, and geometry under the domain of video analysis. But at the same time using deep learning for these tasks becomes infeasible due to the huge requirement of labeled video data. Anyway, to achieve high performance, large-scale tracking datasets become necessary which in turn requires extensive efforts and thus makes the deep learning approach more impractical and expensive. Keeping this in mind, recent researchers have put their faith in a promising approach to make the machines learn without human supervision (labeled data) by leveraging large amounts of unlabeled and raw video data. This quest for self-supervised learning started with a research proposal from the Google research team that suggested to make a visual tracking system by training a model on a proxy task of video colorization that doesn’t require any additional labeled data (self-supervision). However, the research suggested that instead of making the model predict the color of the input grayscale frame, it must learn to copy the colors from a set of reference frame, thus leading to the rise of a pointing mechanism that is able to track the spatial feature of a video sequence in a temporal setup. Visualizations and experiments of these self-supervised methods suggest that, although the network is trained without any human supervision, a mechanism for visual feature tracking automatically emerges inside the network. After plenty of training on unlabeled video collected from the internet, the self-supervised model was able to track any segmented region specified in the initial frame of the video frame sequence. However, the self-supervised deep learning methods are trained on an assumption that the color in the frame sequence is temporally stable. Clearly, there are exceptions, like colorful lights can turn on and off in the video.\n<figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/page/static/3d434ec0cd5d26f818d95625c1a39215/8c557/dofe4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 53.37837837837838%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsSAAALEgHS3X78AAACoklEQVQozzWO209SAQDGz3+ivfRQulVqYlRmqI0gjwuMeZABBxPxcOdwuBxALgOd5JaKFzRnzlvMWWZbW9ZDs1hq88XE0Zy3RSIJtakInCOeXKxv3+P3+/YDCII4PT2NRqOrq6vb21uh0Np+dI8kCZLMUNTZebPZbCqVOt/EYrGNjY1wOByPx0mSTKfTgM/nA8Fao9GoQw0vpudwi4PziA8JxNMzc9S/HB4de9xuu92uUChgGGYymQwGo66uzuFwAGazmUajjU9MmVDpSCdsU9dIeUVa8a2pAYyIzBKR17HIdxQz6fWY3z8gEonodDoIgkKh8PwOcLlcZWW0nj6/UQXJeFcaYX5vl4dbW9XV6aSozEl87ffm2zdjNpfD0tfXjxkwpUrpfeLFcdxmswFDw8NFJSWFlwvMFjve2tbd0+9ytl8voXc97f2vnWx36r1tjoWPwcBkANMbVCqNRNxoNuLAl3fzRq1CbdVMjQ2N+XsnRwdRu0bRKpv9MJODDw5+IRpdj69jen48MD9p9+Iuj8naYejsbwdWRkZxHqfZrLRhYjfEc8JsyFJ+B73oe+WhyASViUYTf0RapVrKkovZosb7fDWzHq1m6Isb7ALg68AzXfU9UNnsRlGnsMoCcxHpQ5b06vCEk0rukomVH1vLiIwjhytNfAiB75ZqLxTr8q8p81l4BfB+8DnMZtY0ccUcBBZxZBBXDkGFcJ4rYM1p70Y2HaZSrYQt5dfKILCquaK85UYRkldhKQACM9NsGdigF9gQAYZIlIomtMVaL27yvezOwdHEvkAr0TyWmxDIIJc8MLIqraU3zZfAtttAOpM+Jg7DW+Hg54XQt1BwaXlnZ49MUul0JgefZc+SxykinVpfX19cWgwuf9r5uZ3MJI9ODv8CzE2Dwt4zVcQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Image\"\n        title=\"Image\"\n        src=\"/page/static/3d434ec0cd5d26f818d95625c1a39215/fcda8/dofe4.png\"\n        srcset=\"/page/static/3d434ec0cd5d26f818d95625c1a39215/12f09/dofe4.png 148w,\n/page/static/3d434ec0cd5d26f818d95625c1a39215/e4a3f/dofe4.png 295w,\n/page/static/3d434ec0cd5d26f818d95625c1a39215/fcda8/dofe4.png 590w,\n/page/static/3d434ec0cd5d26f818d95625c1a39215/8c557/dofe4.png 700w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">Image</figcaption>\n  </figure>\nThe objective of self-supervised learning in tracking is to learn feature embedding that is suitable for matching correspondences along the frame sequence of a video. The correspondence flow is learned by exploiting the natural spatial-temporal coherence in the frame sequence. Correspondence flow can be understood as the feature similarity flow existing between consecutive frames. In simple language, this approach learns a pointer mechanism that can reconstruct a target image by copying pixel information from a set of reference frames. Therefore to make such a model, there are certain precautions a researcher must keep in mind while designing the architecture. First, we must prevent the model from learning trivial solution of this task ( e.g. matching consecutive frames based on low-level color features). Second, we must make the tracker drifting less severe. Tracker drifting (TD) is mainly caused due to occlusion of objects, complex object deformation, and random illumination changes. TD is usually handled by training recursive models over long temporal windows with cycle consistency and scheduled sampling.\n<figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/page/static/f9b6e5018a113e3f85dcec43cd3d9d62/29d31/dofe5.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 29.72972972972973%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAGABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAQD/8QAFwEAAwEAAAAAAAAAAAAAAAAAAAECA//aAAwDAQACEAMQAAABtkJMxm//xAAZEAACAwEAAAAAAAAAAAAAAAABAgADESH/2gAIAQEAAQUCNcfhbN//xAAWEQEBAQAAAAAAAAAAAAAAAAAAATH/2gAIAQMBAT8BXX//xAAWEQEBAQAAAAAAAAAAAAAAAAAAARH/2gAIAQIBAT8BrX//xAAWEAADAAAAAAAAAAAAAAAAAAABECH/2gAIAQEABj8CNf8A/8QAGhAAAwADAQAAAAAAAAAAAAAAAAERIVFhkf/aAAgBAQABPyFjdd4RD9WFKSfp/9oADAMBAAIAAwAAABCAL//EABYRAQEBAAAAAAAAAAAAAAAAAAEAEf/aAAgBAwEBPxACOIv/xAAXEQADAQAAAAAAAAAAAAAAAAAAAREh/9oACAECAQE/EHamlH//xAAaEAEBAAMBAQAAAAAAAAAAAAABEQAhMVFh/9oACAEBAAE/ECuGoBIeXLUE5B3z7zBMwhau5//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Image\"\n        title=\"Image\"\n        src=\"/page/static/f9b6e5018a113e3f85dcec43cd3d9d62/1c72d/dofe5.jpg\"\n        srcset=\"/page/static/f9b6e5018a113e3f85dcec43cd3d9d62/a80bd/dofe5.jpg 148w,\n/page/static/f9b6e5018a113e3f85dcec43cd3d9d62/1c91a/dofe5.jpg 295w,\n/page/static/f9b6e5018a113e3f85dcec43cd3d9d62/1c72d/dofe5.jpg 590w,\n/page/static/f9b6e5018a113e3f85dcec43cd3d9d62/29d31/dofe5.jpg 700w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">Image</figcaption>\n  </figure>\nFinally, before we look under the hood of this pointer mechanism, let’s cover some of the above-mentioned points that one must consider while designing such models. First, it’s important to remember that correspondence matching is the fundamental building block of these models. Therefore there is a high probability that the model will learn a trivial solution while doing frame reconstruction by pixel-wise matching. To prevent the model from overfitting on a trivial solution, it is important to add color jittering and channel-wise dropout, so that model is forced to rely on low-level color information and must be robust to any kind of color jittering. Lastly, to handle TD, as suggested earlier, recursive training over long temporal windows with forward-backward consistency and scheduled sampling is the best way to alleviate the tracker drifting problem. If we apply the above-mentioned methods, we can be sure that the model robustness will increase and the approach will be able to exploit the spatial-temporal coherence of the video and colors will be able to act as a reliable supervision signal for learning correspondences.</p>\n<h2 id=\"Self-supervised-Attention-under-the-Hood\" style=\"position:relative;\"><a href=\"#Self-supervised-Attention-under-the-Hood\" aria-label=\"Self supervised Attention under the Hood permalink\" class=\"gatsby-remark-autolink before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Self-supervised Attention under the Hood</h2>\n<p>If you look deeper into what actually is the pointer mechanism that is being learned here, you will come to the conclusion that it is a type of attention mechanism. Yes, it’s ultimately the famous trio of QKV (Query-Key-Value, the basis of most attention mechanisms).\n<figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/page/static/0f1bc3e5863dea21f01206ae802e752b/8c557/dofe6.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 18.243243243243242%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAIAAAABPYjBAAAACXBIWXMAAAsSAAALEgHS3X78AAAAjUlEQVQI132P3QqEIBSEff8XFBL/Ao22zFw1LMzdwa53v4vDnGEcjuTzm9bafd8Qx3FYa8dxfHWWZdFaT9NE3p0QAmZK6bqufd9jjNu2IQFxnmfO2RhDKRVCKKXWdZ3nGY0EIe+9cw7vsSMKDRMVUkqYqIMJgVjo1Fqf08j/sx9KKcMwcM4ZY5j4C0wEvr2+5ByoS9I7AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Image\"\n        title=\"Image\"\n        src=\"/page/static/0f1bc3e5863dea21f01206ae802e752b/fcda8/dofe6.png\"\n        srcset=\"/page/static/0f1bc3e5863dea21f01206ae802e752b/12f09/dofe6.png 148w,\n/page/static/0f1bc3e5863dea21f01206ae802e752b/e4a3f/dofe6.png 295w,\n/page/static/0f1bc3e5863dea21f01206ae802e752b/fcda8/dofe6.png 590w,\n/page/static/0f1bc3e5863dea21f01206ae802e752b/8c557/dofe6.png 700w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">Image</figcaption>\n  </figure>\nAs we know, the goal of the self-supervised model is to learn robust correspondence matching by effectively encoding feature representations. In simple language, the ability to copy effectively is achieved by training on a proxy task, where the model learns to reconstruct a target frame by linearly combining pixel data from the reference frames, with the weights measuring the strength of correspondence between pixels. However, breaking down this process, we find that there is a triplet (Q, K, V) for every input frame we process. The Q, K, V refer to Query, Key, and Value. To reconstruct a pixel I¹ in the T¹ frame, an Attention mechanism is used for copying pixels from a subset of previous frames in the original sequence. Just, in this case, the query vector (Q) is the present frame’s(I¹) feature embedding (target frame), the key Vector is the previous frame’s(I⁰) feature embedding (reference frame). Now if we compute a dot product (.) between the query and key (Q.K) and take a softmax of the computed product, we can get a similarity between the present frame ( I¹ ) and the previous reference frame (I⁰). This computed similarity matrix when multiplied with a reference instance segmentation mask (V) during inference will give us a pointer for our target frame, thus achieving dense optical flow estimation. Therefore this pointer which is just a combination of Q, K, and V is the actual attention mechanism working under the hood of this self-supervised system.\nA key element in attention mechanism training is to establish a proper information bottleneck. To circumvent any learning shortcuts that the attention mechanism may resort to, the previously mentioned techniques of intentionally dropping the input color information and channel dropout are used. However, the choice of color spaces still plays an important role in training these attention mechanisms through self-supervision. Many research works have validated the conjecture that using decorrelated color space leads to better feature representations for self-supervised dense optical flow estimation. In simple language, using the LAB format image works better than the RGB format. This is because all RGB channels include a representation of brightness, making it highly correlate to the luminance in Lab, therefore acting as a weak information bottleneck.</p>\n<h2 id=\"Restricted-Attention-for-minimizing-physical-memory-costs\" style=\"position:relative;\"><a href=\"#Restricted-Attention-for-minimizing-physical-memory-costs\" aria-label=\"Restricted Attention for minimizing physical memory costs permalink\" class=\"gatsby-remark-autolink before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Restricted Attention for minimizing physical memory costs</h2>\n<p>The above-proposed attention mechanism usually comes with high physical memory cost. Therefore processing high-resolution information for correspondence matching can lead to large memory requirements and slower speed.\n<figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 460px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/page/static/1d412681ecbb0ca49be51324e54ca1a5/e41a8/dofe7.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 65.54054054054055%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAQBAgP/xAAVAQEBAAAAAAAAAAAAAAAAAAABAv/aAAwDAQACEAMQAAABI1vKgPlH/8QAGRAAAgMBAAAAAAAAAAAAAAAAAREAAhIx/9oACAEBAAEFAic0d9GqPYQE5//EABURAQEAAAAAAAAAAAAAAAAAABAh/9oACAEDAQE/AYf/xAAWEQEBAQAAAAAAAAAAAAAAAAAAESH/2gAIAQIBAT8B1X//xAAcEAADAAEFAAAAAAAAAAAAAAAAARECEBIhcaH/2gAIAQEABj8CspM0tr8IzlHen//EABgQAAMBAQAAAAAAAAAAAAAAAAERIQBh/9oACAEBAAE/IQOCTQB7uQQZKLpDMFQaW4IBv//aAAwDAQACAAMAAAAQrB//xAAWEQEBAQAAAAAAAAAAAAAAAAAREDH/2gAIAQMBAT8QRk//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/EA//xAAdEAEAAwABBQAAAAAAAAAAAAABABEhMWFxgZGh/9oACAEBAAE/EDLFTLFofK5lzqvC6dO09z0QFQDpFZMBzeIVUGRXiVJBw5n/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Image\"\n        title=\"Image\"\n        src=\"/page/static/1d412681ecbb0ca49be51324e54ca1a5/e41a8/dofe7.jpg\"\n        srcset=\"/page/static/1d412681ecbb0ca49be51324e54ca1a5/a80bd/dofe7.jpg 148w,\n/page/static/1d412681ecbb0ca49be51324e54ca1a5/1c91a/dofe7.jpg 295w,\n/page/static/1d412681ecbb0ca49be51324e54ca1a5/e41a8/dofe7.jpg 460w\"\n        sizes=\"(max-width: 460px) 100vw, 460px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">Image</figcaption>\n  </figure>\nTo circumvent the memory cost, ROI localization is used to estimate the candidate windows non-locally from memory banks. Intuitively, we can say that for temporally close frames, spatial-temporal coherence naturally exists in the frame sequence. This ROI localization leads to restricted attention as now the pixel in the target frame is only compared to spatially neighboring pixels of the reference frame. The number of comparable pixels is determined by the size of the dilated window in which the attention is restricted. The dilation rate of the window is proportional to the temporal distance between the present frame and the past frames in the memory bank. After computing the affinity matrix of the restricted attention region, fine-grained matching scores can be computed in a non-local manner. Therefore, with the proposed memory-augmented restricted attention mechanism, the model can efficiently process high-resolution information without incurring large physical memory costs.</p>\n<h2 id=\"Conclusion\" style=\"position:relative;\"><a href=\"#Conclusion\" aria-label=\"Conclusion permalink\" class=\"gatsby-remark-autolink before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>In this blog, we started with an introduction to the concept of optical flow and studied its application in object tracking. We also studied how this concept inspired the deep learning tracking systems and how self-supervision and visual attention plays a key role in making these systems. The computed optical flow vectors open a myriad of possible applications that require such an in-depth scene understanding of videos. The discussed techniques are majorly applied to pedestrian tracking, autonomous vehicle navigation, and many more novel applications. The variety of applications where the optical flow can be applied is only limited by the ingenuity of its designers.\nIn my personal opinion, self-supervision will soon serve as a strong competitor to its supervised counterpart because of its generalizability and flexibility. Self-supervision easily outperforms most of the supervised methods on unseen object categories, which reflects its importance and power in the coming time as we take our steps towards solving human intelligence.\nMy blogs are a reflection of what I worked on and simply convey my understanding of these topics. My interpretation of deep learning can be different from that of yours, but my interpretation can only be as inerrant as I am.</p>\n<h3 id=\"Read-more-\" style=\"position:relative;\"><a href=\"#Read-more-\" aria-label=\"Read more  permalink\" class=\"gatsby-remark-autolink before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Read more 📖</h3>\n<ul>\n<li><a href=\"https://rishab.co/blog\">Blog</a></li>\n</ul>\n<p>🙏 Thanks for reading! hope you liked the post.\nHave a nice day bye! 👋</p>\n<p><em>Comment down if you want to know more</em></p>","timeToRead":12,"id":"dba66325-d217-5d8f-8f0e-a9d6573faf85","frontmatter":{"date":"July 18, 2020","title":"Self-supervised Attention Mechanism for Dense Optical Flow Estimation"}}},"pageContext":{"slug":"/blog/self-supervised-attention-mechanism-for-dense-optical-flow-estimation"}},"staticQueryHashes":["1033876704","3435786681","550521386"]}