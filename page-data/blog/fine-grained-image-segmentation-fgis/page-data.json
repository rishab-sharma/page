{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/fine-grained-image-segmentation-fgis","result":{"data":{"markdownRemark":{"excerpt":"The Problem and Computer Vision To The Rescue Nowadays, realistic editing of photographs demands a careful treatment of color blends that…","html":"<h2 id=\"The-Problem-and-Computer-Vision-To-The-Rescue\" style=\"position:relative;\"><a href=\"#The-Problem-and-Computer-Vision-To-The-Rescue\" aria-label=\"The Problem and Computer Vision To The Rescue permalink\" class=\"gatsby-remark-autolink before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>The Problem and Computer Vision To The Rescue</h2>\n<p>Nowadays, realistic editing of photographs demands a careful treatment of color blends that frequently occur in natural scenes. These color blends are typically modeled through a soft selection of scene or object colors. Therefore to achieve a high-quality image editing and background composition, accurate representation of these soft transitions between image regions is essential. The majority of the present techniques used in the industry for generating such representations are heavily dependent on some kind of user interaction by skilled visual artists. So creating such accurate saliency selection becomes an expensive and tedious task. To fulfill this void of skilled visual artists, we utilize computer vision to simulate the human vision system which has an effective attention mechanism for determining the most salient information from visual scenes. Such a kind of problem can also be interpreted as a foreground extraction problem, where the salient objects are considered as a foreground class and the remaining scene is the background class. Computer vision and deep learning aim to model such a mechanism through some selective research branches namely image matting, salient object detection, eye-fixation detection, and soft segmentation. It is also important to note that unlike computer vision, deep learning is mostly a data-intensive research approach.\nWith the recent rise in the use of fully convolutional networks (FCN) for image segmentation, deep learning has significantly improved the foreground extraction and saliency detection baselines. Despite all these improvements, most of the suggested architectures use network backbones that were originally designed for image attribute classification tasks, which extract representative features that are of semantic signification rather than global contrast and local detail information. But in this blog, we will leave this issue to be discussed in more detail in my later blogs.</p>\n<h2 id=\"Is-it-a-Segmentation-problem\" style=\"position:relative;\"><a href=\"#Is-it-a-Segmentation-problem\" aria-label=\"Is it a Segmentation problem permalink\" class=\"gatsby-remark-autolink before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Is it a Segmentation problem?</h2>\n<p>Yes, it’s a segmentation problem if we look at it from the angle of our output format. In recent years, semantic segmentation has become a key problem in the field of computer vision and deep learning. Thus looking at the bigger scenario, we can say that semantic segmentation is one of the key tasks in its field that paves the way towards better scene understanding. The significance of scene understanding is also highlighted by the evidence of an increasing number of applications that nourishes from inferring cognitive facts from images and videos.\nThe three approaches we discuss for achieving a smooth and perceptually sound fine-grained semantic segmentation are:</p>\n<ul>\n<li>Image Matting</li>\n<li>Salient Object Detection (SOD)</li>\n<li>Soft Segmentation</li>\n</ul>\n<h2 id=\"Image-Matting\" style=\"position:relative;\"><a href=\"#Image-Matting\" aria-label=\"Image Matting permalink\" class=\"gatsby-remark-autolink before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Image Matting</h2>\n<p>Image matting can be understood as a generalized version of green-screen keying used for precise estimation of foreground opacities in an unconstrained setting. Image matting is a very important topic in both computer graphics and vision applications. Earlier approaches to image matting involved large sparse matrices such as large kernel matting Laplacian and its optimization. However, these methodologies of solving such linear systems are often very time consuming and unfavoured by users. Many pieces of research tried to improve this linear system solving speed by using adaptive kernel sizes and KD-tree, but no significant improvement was observed in terms of quality on wild images and inference speed. Since the problem is highly ill-posed, a trimap (or strokes) indicating definite foreground, definite background, and unknown regions is usually given by the user as a supportive input.</p>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/page/static/afe18a1d8a0a7d7a694b555fd850482d/29d31/fgis1.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 33.108108108108105%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAHABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAMF/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAbxIzRX/xAAZEAACAwEAAAAAAAAAAAAAAAAEEgABAgP/2gAIAQEAAQUCwuZxpjzaUr//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAaEAACAgMAAAAAAAAAAAAAAAAAEgECESFB/9oACAEBAAY/AsU1A3VYuf/EABsQAQEAAgMBAAAAAAAAAAAAAAERACExUWFx/9oACAEBAAE/Id+tKj2YBMKse8YYACx18z//2gAMAwEAAgADAAAAEIfP/8QAFxEBAAMAAAAAAAAAAAAAAAAAARARIf/aAAgBAwEBPxBbyP/EABYRAQEBAAAAAAAAAAAAAAAAAAEREP/aAAgBAgEBPxCQHP/EABsQAQACAgMAAAAAAAAAAAAAAAEAESExQWGx/9oACAEBAAE/EAOT6Vtlb51BFN5NMMPYBFVPYZn/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Image\"\n        title=\"Image\"\n        src=\"/page/static/afe18a1d8a0a7d7a694b555fd850482d/1c72d/fgis1.jpg\"\n        srcset=\"/page/static/afe18a1d8a0a7d7a694b555fd850482d/a80bd/fgis1.jpg 148w,\n/page/static/afe18a1d8a0a7d7a694b555fd850482d/1c91a/fgis1.jpg 295w,\n/page/static/afe18a1d8a0a7d7a694b555fd850482d/1c72d/fgis1.jpg 590w,\n/page/static/afe18a1d8a0a7d7a694b555fd850482d/29d31/fgis1.jpg 700w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">Image</figcaption>\n  </figure></p>\n<p>Let’s first formulate a basic equation for image matting. Denoting an image pixel’s background color, foreground color, and foreground opacity (alpha matte) as B, F and α respectively, the pixel’s color C can be written as a convex combination of B and F:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">C = F (α)+ B(1 − α).</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n<p>Image matting methods can be categorized into three major types, propagation-based, sampling-based, and learning-based. In some approaches, a hybrid combination of sampling-based and propagation-based matting is also used.\nSampling-based image matting is based on an assumption that the true background and foreground colors of the uncharted pixels can be derived from the known background and foreground pixels that are localized near that unknown pixel. Some sampling-based methods are:</p>\n<ul>\n<li>Shared sampling matting</li>\n<li>Iterative matting</li>\n<li>Bayesian matting</li>\n<li>Sparse coding\nPropagation-based image matting techniques calculate the alpha values of the uncharted pixels by propagating the alpha values of the known local background and foreground pixels into the unknown regions. However, in the case of wild background images, the over-dependency on color knowledge leads to artifacts in images where the distribution of background and foreground color overlap. Some propagation-based methods are:</li>\n<li>Geodesic matting</li>\n<li>Close-form matting</li>\n<li>Poisson matting</li>\n<li>Spectral matting\nNonetheless, both sampling and propagation-based techniques are unable to provide satisfactory and completely automated results. Thus, recently, several deep learning studies have suggested approaches that either solve the above mentioned linear system through a concatenated input of trimap and RGB image into FCN or just the RGB image itself to predict final alpha matte. Some known trimap dependent deep learning architectures are:\nDeep Automatic Portrait Matting by Shen et al.\nDeep Image Matting ( DIM ) by Xu et al.\nDisentangled Image Matting by Cai et al.\nWhereas some trimap independent deep learning architectures are:\nLate Fusion Matting by Zhang et el.\nSemantic Human Matting by Chen et al.\nAlphaNet: An Attention Guided Deep Network for Automatic Image Matting by Sharma et al.\n<figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/page/static/70892a05b1c5fed6cb9fe3a30fcd105b/29d31/fgis2.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 36.486486486486484%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAHABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAdiEUV//xAAZEAABBQAAAAAAAAAAAAAAAAAAAQIREyP/2gAIAQEAAQUCl4lhof/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABgQAQEAAwAAAAAAAAAAAAAAAAEAETFC/9oACAEBAAY/AtE5C5v/xAAaEAACAwEBAAAAAAAAAAAAAAABEQAhMVGR/9oACAEBAAE/ISoVdvIXgUdl8Xrc/9oADAMBAAIAAwAAABD/AM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAcEAEAAgEFAAAAAAAAAAAAAAABESEAMUFhgeH/2gAIAQEAAT8QOlixaF+ZDSU3h2rFI0pAKviHP//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Image\"\n        title=\"Image\"\n        src=\"/page/static/70892a05b1c5fed6cb9fe3a30fcd105b/1c72d/fgis2.jpg\"\n        srcset=\"/page/static/70892a05b1c5fed6cb9fe3a30fcd105b/a80bd/fgis2.jpg 148w,\n/page/static/70892a05b1c5fed6cb9fe3a30fcd105b/1c91a/fgis2.jpg 295w,\n/page/static/70892a05b1c5fed6cb9fe3a30fcd105b/1c72d/fgis2.jpg 590w,\n/page/static/70892a05b1c5fed6cb9fe3a30fcd105b/29d31/fgis2.jpg 700w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">Image</figcaption>\n  </figure>\nIn my personal experience, deep learning-based approaches are able to capture global semantic information and local details better than the other two approaches, also they are not biased on any crude assumption of the existence of a correlation between the known and unknown region pixels.</li>\n</ul>\n<h2 id=\"Salient-Object-Detection-SOD\" style=\"position:relative;\"><a href=\"#Salient-Object-Detection-SOD\" aria-label=\"Salient Object Detection SOD permalink\" class=\"gatsby-remark-autolink before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Salient Object Detection (SOD)</h2>\n<p>SOD's major objective is to segment the most salient (important) and visually attractive object in the picture. Many fields such as image segmentation and visual tracking apply SOD in a variety of applications. Similar to image matting, after the rise of Fully Convolutional Networks (FCN) for saliency detection, SOD state-of-the-art has improved significantly.\n<figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/page/static/cfe6d7a1fbd7a2d9421526132f606b4c/29d31/fgis3.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 35.810810810810814%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAHABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAMEBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAG3nkQCv//EABsQAAIBBQAAAAAAAAAAAAAAAAMEAQIFEhMy/9oACAEBAAEFAj8xTtuLocWP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFREBAQAAAAAAAAAAAAAAAAAAARD/2gAIAQIBAT8BJ//EABkQAQACAwAAAAAAAAAAAAAAAAEAIRIxYf/aAAgBAQAGPwItJzccSm5//8QAGhABAAIDAQAAAAAAAAAAAAAAAQARITFBcf/aAAgBAQABPyHKhA8YOkTSvyLQqcDk/9oADAMBAAIAAwAAABCHz//EABcRAQADAAAAAAAAAAAAAAAAAAEQESH/2gAIAQMBAT8QXaj/xAAWEQADAAAAAAAAAAAAAAAAAAABECH/2gAIAQIBAT8QCL//xAAdEAEAAQMFAAAAAAAAAAAAAAABABEhQTFhcYGh/9oACAEBAAE/EFZoUWL4d8zQ7BGogcc0jOEhoAmz6L3P/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Image\"\n        title=\"Image\"\n        src=\"/page/static/cfe6d7a1fbd7a2d9421526132f606b4c/1c72d/fgis3.jpg\"\n        srcset=\"/page/static/cfe6d7a1fbd7a2d9421526132f606b4c/a80bd/fgis3.jpg 148w,\n/page/static/cfe6d7a1fbd7a2d9421526132f606b4c/1c91a/fgis3.jpg 295w,\n/page/static/cfe6d7a1fbd7a2d9421526132f606b4c/1c72d/fgis3.jpg 590w,\n/page/static/cfe6d7a1fbd7a2d9421526132f606b4c/29d31/fgis3.jpg 700w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">Image</figcaption>\n  </figure>\nUnlike Natural Image Matting, salient object detection is not as complex as it seems. The major challenges in achieving a precise salient object detection are:\n(1) The Saliency Localization. The saliency of a particular visual asset is generally defined over the global contrast of the entire image rather than any pixel-wise or local feature. Therefore, in order to achieve a precise SOD, the saliency detection algorithm must not only capture the global contrast of the whole image but also establish an accurate representation of the detailed structures of the foreground object. To tackle this problem, multi-level deep feature aggregation networks are used.\n(2) The Absence of Boundary Refinement Losses. The most common loss used for training saliency object detection models is either Intersection over Union (IoU) loss or Cross-Entropy (CE). But both of them lead to a blurry boundary detailing due to their incipiency of efficiently differentiating boundary pixels. Many studies also use the Dice-score loss, but its major purpose is to handle the biased training sets and not specifically enforcing the modeling of fine structures.</p>\n<h3 id=\"Research-History\" style=\"position:relative;\"><a href=\"#Research-History\" aria-label=\"Research History permalink\" class=\"gatsby-remark-autolink before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Research History</h3>\n<p>There has been a rich modern history of deep learning literature for salient object detection. Some studies emphasize on the use of deep recurrent networks with attention mechanisms for iterative refinement of some selective image sub-regions. On the other hand, some studies highlight the effectiveness of global information transfer from the deep layers of the network to the shallow end by a deep multi-path recurrent connection. Many authors like Hu et al.[1] and Wang et al.[2] proposed methods that either uses recurrent fully connected networks or recurrently concatenated multi-layer deep features for salient object detection. These studies also show the effectiveness of iterative correction of prediction errors. In contrast to the previously mentioned research works, a few pieces of research also showed the use of a contextual attention network in U-Net architecture for predicting the pixel-wise attention maps. These extracted pixel-wise attention maps are proved very effective for saliency detection in terms of evaluation metrics. Few proposed methods emphasize on coarse to fine prediction transitioning. These methods propose refinement strategies for achieving more accurate boundary details by capturing finer structures. For example, Lu et al. proposed an architecture that captures a deep hierarchical salient representation for modeling various global structured saliency cues of the saliency maps along with a post-refinement stage. The latest published advancement in the field of salient object detection (as I write this blog) is by Qin et al. who suggest a powerful deep network architecture (U2-Net) with a two-level nested U-structure. The key improvements stated by the authors are multi-scale contextual information captivity (a mixture of receptive fields) and increased network depth (pooling in ReSidual U-blocks) without any significant computational expense.\n<figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 545px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/page/static/2ecdcf67a37a76c92c5e108a70553ca7/95118/fgis4.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 50%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAMEBf/EABYBAQEBAAAAAAAAAAAAAAAAAAEAAv/aAAwDAQACEAMQAAABWvPXmtIBP//EABwQAAEEAwEAAAAAAAAAAAAAAAEAAgMREhMhMv/aAAgBAQABBQLbkmzR0ZRbfQPCV//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABYRAAMAAAAAAAAAAAAAAAAAAAABEf/aAAgBAgEBPwFqkP/EABwQAAEDBQAAAAAAAAAAAAAAAAABETEQIUFxkf/aAAgBAQAGPwKFh4LuZOi6p//EAB4QAQEAAgAHAAAAAAAAAAAAAAERADEhQVFhcYGR/9oACAEBAAE/ISgjNdCTlmkL1miQecamtgD44xBS4R2rOB2z/9oADAMBAAIAAwAAABAI3//EABcRAAMBAAAAAAAAAAAAAAAAAAABESH/2gAIAQMBAT8QWOkP/8QAFhEBAQEAAAAAAAAAAAAAAAAAAQAx/9oACAECAQE/ED3AL//EABwQAQEAAgIDAAAAAAAAAAAAAAERACExUWGR8P/aAAgBAQABPxColWKjQvYouMS32Jn28sLpU2LvnBRgIWw4D3gYZBBl3iYCGC2Ojxn/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Image\"\n        title=\"Image\"\n        src=\"/page/static/2ecdcf67a37a76c92c5e108a70553ca7/95118/fgis4.jpg\"\n        srcset=\"/page/static/2ecdcf67a37a76c92c5e108a70553ca7/a80bd/fgis4.jpg 148w,\n/page/static/2ecdcf67a37a76c92c5e108a70553ca7/1c91a/fgis4.jpg 295w,\n/page/static/2ecdcf67a37a76c92c5e108a70553ca7/95118/fgis4.jpg 545w\"\n        sizes=\"(max-width: 545px) 100vw, 545px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">Image</figcaption>\n  </figure>\nIn my personal experience, SOD also achieves higher quality saliency maps as natural image matting but an inferior quality in terms of transparency modeling and fine-structure extraction.</p>\n<h2 id=\"Soft-segmentation\" style=\"position:relative;\"><a href=\"#Soft-segmentation\" aria-label=\"Soft segmentation permalink\" class=\"gatsby-remark-autolink before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Soft segmentation</h2>\n<p>Soft segmentation is defined as the decomposition of the image into two or more sections where each member pixel may own membership into two or more sections.\n<img src=\"./images/fgis5.jpeg\" alt=\"Image\"></p>\n<h3 id=\"Research-History-1\" style=\"position:relative;\"><a href=\"#Research-History-1\" aria-label=\"Research History 1 permalink\" class=\"gatsby-remark-autolink before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Research History</h3>\n<p>Most of the earlier soft segmentation methods emphasize on the extraction of soft saliency maps of various homogeneous colors using either per-pixel color unmixing or global optimization. Although these extracted soft color maps are observed to be useful for many critical image editing applications such as image recoloring but similar to SOD they do not specifically respect object boundary and transition region granularity. It is interesting to note that image matting has a very close relationship with the branch of soft segmentation. In fact, some pieces of image matting literature such as Matting Laplacian are completely aligned with the key idea of soft segmentation, which is capturing a powerful representation for the local soft transition regions in the image. Given a set of user-defined regions, these methods mainly work on the idea of iteratively solving a two-layer soft segmentation problem to generate multiple layers. The work of Levin et al. on spectral matting also serves the same purpose by estimating a set of spatially connected soft segments automatically via spectral decomposition. Recent soft segmentation research by Aksoy et al. also follows the idea of spectral matting in combination with spectral decomposition and matting Laplacian. However, unlike spectral matting, their work approaches the problem from a spectral decomposition angle by fusing the local texture information with the high-level features from a deep convolutional neural network trained for scene analysis. One of their key contributions is the use of the graph like structure to enrich the eigenvectors of the corresponding Laplacian matrix by semantic objects as well as the soft transitions between them.\n<figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 338px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/page/static/45cb70b527d23ffe9a876a6cf1e64d97/9c05e/fgis6.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 43.91891891891892%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAJABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAMEAf/EABYBAQEBAAAAAAAAAAAAAAAAAAIBA//aAAwDAQACEAMQAAABViL4ZiMOn//EABsQAAIDAAMAAAAAAAAAAAAAAAIDAAEEFDQ1/9oACAEBAAEFAuQQBbjGFrYq470tfa//xAAYEQACAwAAAAAAAAAAAAAAAAAAAQIREv/aAAgBAwEBPwHKFKj/xAAXEQEBAQEAAAAAAAAAAAAAAAABAAIR/9oACAECAQE/AeE4G//EAB4QAAIBAwUAAAAAAAAAAAAAAAABAwISIQQRUnGB/9oACAEBAAY/Ak9nbWKN0vOSziaYj8JOz//EAB4QAAICAAcAAAAAAAAAAAAAAAERACEQMUFRkaHw/9oACAEBAAE/ISlJye0s1VDW4UqBksOe8zq4I//aAAwDAQACAAMAAAAQ9y//xAAYEQEBAQEBAAAAAAAAAAAAAAABEQBBkf/aAAgBAwEBPxCBJyZAAHm//8QAGBEBAQADAAAAAAAAAAAAAAAAAQBRYaH/2gAIAQIBAT8Q0Y5JKl//xAAdEAABBAIDAAAAAAAAAAAAAAABABEhMWHwUbHx/9oACAEBAAE/ECbqLEhbTF2mwAAxcyQbxEAbKHUZdHoV3XJWrgL/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Image\"\n        title=\"Image\"\n        src=\"/page/static/45cb70b527d23ffe9a876a6cf1e64d97/9c05e/fgis6.jpg\"\n        srcset=\"/page/static/45cb70b527d23ffe9a876a6cf1e64d97/a80bd/fgis6.jpg 148w,\n/page/static/45cb70b527d23ffe9a876a6cf1e64d97/1c91a/fgis6.jpg 295w,\n/page/static/45cb70b527d23ffe9a876a6cf1e64d97/9c05e/fgis6.jpg 338w\"\n        sizes=\"(max-width: 338px) 100vw, 338px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">Image</figcaption>\n  </figure>\nIn my personal experience, soft segmentation is a derived branch of natural image matting that combines the rich history image matting practices with the power of deep learning. Also unlike vanilla image matting, soft segmentation gives more layers of output that represent the semantically meaningful regions. But despite all these significant improvements, there remains a huge scope of improvement that still needs to be solved.</p>\n<h2 id=\"Conclusion\" style=\"position:relative;\"><a href=\"#Conclusion\" aria-label=\"Conclusion permalink\" class=\"gatsby-remark-autolink before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>I have explained these approaches from a point of view of solving the problem of salient foreground extraction, but the actual problem which these methods aim to solve is very diverse and enriched in their respective research branches and contribute to the domain of Deep Computer Vision (my take on Computer Vision + Deep learning) in their way.</p>\n<p>This blog gives an overview of all these approaches and honestly from a researcher's perspective, we have not even properly scratched the surface of these topics. To read in detail about matting check out AlphaNet, and I may discuss these topics in a more dive deep manner in my future blogs.</p>\n<h3 id=\"Read-more-\" style=\"position:relative;\"><a href=\"#Read-more-\" aria-label=\"Read more  permalink\" class=\"gatsby-remark-autolink before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Read more 📖</h3>\n<ul>\n<li><a href=\"https://rishab.co/blog\">Blog</a></li>\n</ul>\n<p>🙏 Thanks for reading! hope you liked the post.\nHave a nice day bye! 👋</p>\n<p><em>Comment down if you want to know more</em></p>","timeToRead":9,"id":"918990bc-acd8-5530-b563-7f3567c1092f","frontmatter":{"date":"May 30, 2020","title":"Fine-Grained Image Segmentation (FGIS)"}}},"pageContext":{"slug":"/blog/fine-grained-image-segmentation-fgis"}},"staticQueryHashes":["1033876704","3435786681","550521386"]}