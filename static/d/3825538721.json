{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"7433d6fa-1144-56c4-87ae-dc2ff565cb1b","frontmatter":{"excerpt":"In this paper, we propose an end to end solution for image matting i.e high-precision extraction of foreground objects from natural images. Image matting and background detection can be achieved easily through chroma keying in a studio setting when the background is either pure green or blue. Nonetheless, image matting in natural scenes with complex and uneven depth backgrounds remains a tedious task that requires human intervention. To achieve complete automatic foreground extraction in natural scenes, we propose a method that assimilates semantic segmentation and deep image matting processes into a single network to generate detailed semantic mattes for image composition task. The contribution of our proposed method is two-fold, firstly it can be interpreted as a fully automated semantic image matting method and secondly as a refinement of existing semantic segmentation models. We propose a novel model architecture as a combination of segmentation and matting that unifies the function of upsampling and downsampling operators with the notion of attention.","iframe":"//arxiv.org/pdf/2003.03613.pdf","src":"//arxiv.org/abs/2003.03613","title":"AlphaNet- An Attention Guided Deep Network for Automatic Image Matting"},"fields":{"slug":"/case-studies/alpha-net-an-attention-guided-deep-network-for-automatic-image-matting"}}},{"node":{"id":"c705ae9c-d681-5fd8-bf0a-15a5fffad975","frontmatter":{"excerpt":"In the history of artificial neural networks, LSTMs have proved to be a high-performance architecture at sequential data learning. Although LSTMs are remarkable in learning sequential data but are limited in their ability to learn long-term dependencies and representation of certain data structures because of the lack of external memory. In this paper, we tackled two main tasks, one is language translation and other is image captioning. We approached the problem of language translation by leveraging the capabilities of the recently developed DNC architectures. Here we modified the DNC architecture by including dual neural controllers instead of one and an external memory module. Inside our controller, we employed a neural network with memory-augmentation which differs from the original differentiable neural computer, we implemented a dual controllerâ€™s system in which one controller is for encoding the query sequence whereas another controller is for decoding the translated sequences.","iframe":"//sciencedirect.com/science/article/pii/S1877050920315325/pdf?md5=a52136479476cc66a52994a1cc967006&pid=1-s2.0-S1877050920315325-main.pdf","src":"//sciencedirect.com/science/article/pii/S1877050920315325","title":"Employing Differentiable Neural Computers for Image Captioning and Neural Machine Translation"},"fields":{"slug":"/case-studies/employing-differentiable-neural-computers-for-image-captioning-and-neural-machine-translation"}}},{"node":{"id":"327f8e05-5b50-53d7-b682-de66c424b9a0","frontmatter":{"excerpt":"In this paper, we propose a deep convolutional neural network for learning the embeddings of images in order to capture the notion of visual similarity. We present a deep siamese architecture that when trained on positive and negative pairs of images learn an embedding that accurately approximates the ranking of images in order of visual similarity notion.","iframe":"//arxiv.org/pdf/1901.03546.pdf","src":"//arxiv.org/abs/1901.03546","title":"Retrieving Similar E-Commerce Images Using Deep Learning"},"fields":{"slug":"/case-studies/retrieving-similar-e-commerce-images-using-deep-learning"}}}]}}}